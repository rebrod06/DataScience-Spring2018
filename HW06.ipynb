{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Data Science – Homework 6\n",
    "*COMP 5360 / MATH 4100, University of Utah, http://datasciencecourse.net/*\n",
    "\n",
    "Due: Friday, February 23, 11:59pm.\n",
    "\n",
    "In Part 1 of this homework you will scrape github repositories and organize the information in a Pandas dataframe. In Part 2, you will use linear regression to gain meaningful insights. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Data\n",
    "*First Name:* Rebecca\n",
    "<br>\n",
    "*Last Name:* Rodriguez\n",
    "<br>\n",
    "*E-mail:* rebrod06@gmail.com\n",
    "<br>\n",
    "*UID:* U0519396\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# imports and setup \n",
    "from bs4 import BeautifulSoup\n",
    "# you can use either of these libraries to get html from a website\n",
    "import requests\n",
    "import urllib.request\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import scipy as sc\n",
    "import numpy as np\n",
    "\n",
    "import statsmodels.formula.api as sm\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline  \n",
    "plt.rcParams['figure.figsize'] = (10, 6) \n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Scrape Github Repository List and Repository Information using BeautifulSoup\n",
    "In this part you will explore over 2 million Github repositories. You are going to scrape [this repository list](https://github.com/search?o=desc&q=stars%3A%3E1&s=stars&type=Repositories)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Check whether you are permitted to scrape the data\n",
    "Before you start to scrape any website you should go through the terms and services or similar pages of the website. Almost all websites post conditions to use their data. Check the terms of [https://github.com/](https://github.com/) to see whether the site permits you to scrape their data or not. Are you sure you are allowed to scrape?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Interpretation:**\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.2 Download the website\n",
    "\n",
    "Download the **first ten** pages of the list of highly starred repositories here: \n",
    "\n",
    "https://github.com/search?o=desc&q=stars%3A%3E1&s=stars&type=Repositories\n",
    "\n",
    "To avoid sending too many requests to the server, make sure that your querying code is in a sperate cell from anything else, so that you don't have to re-run it needlessly. \n",
    "\n",
    "**Warning:** while we haven't experienced any problems, exepct that github will temporarily block access if you scrape their website relentlessly. Make sure to be responsible. Also, don't do this last minute – if you're blocked for e.g., 24 hours that could impact your timeline.\n",
    "\n",
    "Now read the html text in python and create a soup object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#function that converts a url into a BeautifulSoup object\n",
    "\n",
    "def url_to_soup(url):\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        html = response.read()\n",
    "        html = html.decode('utf-8')\n",
    "\n",
    "    # save the file\n",
    "    with open('git_repos.html', 'w', encoding = 'utf-8') as new_file:\n",
    "        new_file.write(html)\n",
    "\n",
    "    # here it's already a local operation\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    return soup \n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Task 1.2a: download first ten pages of most-starred repositories\n",
    "\n",
    "from time import sleep\n",
    "\n",
    "#list to contain urls of 10 pages\n",
    "pages = []\n",
    "\n",
    "#first and last portion of url-- we'll insert the page number in the loop later\n",
    "begin_link = 'https://github.com/search?o=desc&p='\n",
    "end_link = '&q=stars%3A%3E1&s=stars&type=Repositories'\n",
    "\n",
    "#add each url to the list\n",
    "for i in range(1, 11):\n",
    "    pages.append(begin_link + str(i) + end_link)\n",
    "    \n",
    "#list to contain soup objects of the 10 pages\n",
    "soup_pages = []\n",
    "\n",
    "#create soup objects of each url page and add to list\n",
    "for page in pages[0:6]:\n",
    "    soup_pages.append(url_to_soup(page))\n",
    "\n",
    "#take a break so we don't get blocked\n",
    "sleep(60)\n",
    "\n",
    "for page in pages[6:]:\n",
    "    soup_pages.append(url_to_soup(page))\n",
    "    \n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(soup_pages))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Extract Data\n",
    "\n",
    "Extract the following data for each repository, and create a Pandas Dataframe with a row for each repository and a column for each of these datums. \n",
    "\n",
    "1. Name of the repository\n",
    "+ URL of the repository \n",
    "+ Number of Contributors \n",
    "+ Programming Language Used \n",
    "+ Number of stars \n",
    "+ Number of issues\n",
    "+ Number of forks\n",
    "+ Length of readme file.\n",
    "\n",
    "Note that you will frist have to extract links from the soup that you just scraped earlier, and then download the repository pages to retreive most of the data. \n",
    "\n",
    "Refer to lecture 12 for details on how to do this. Make sure to use the web inspector to identify the relevant structures. \n",
    "\n",
    "Save the dataframe you created to a new file `project_info.csv` and include this in your submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#TESTING/DEBUGGING: gathering data by looping through list of page soup objects\n",
    "\n",
    "#first portion of all repo urls\n",
    "url_head = \"https://github.com\"\n",
    "\n",
    "#lists to contain names, urls, issues, lang, stars\n",
    "url_list = []\n",
    "repo_names = []  #we'll need to remove the '/' from beginning of all names\n",
    "language_list = []\n",
    "stars_list = []\n",
    "\n",
    "for i in range(len(soup_pages)):\n",
    "    \n",
    "    soup_thingy1 = soup_pages[i]\n",
    "\n",
    "    #get names and urls of repos\n",
    "    for repo in soup_thingy1.find_all(class_ = \"v-align-middle\"):\n",
    "        #get the tail by retrieving link out the href attribute\n",
    "        link_tail = repo.get(\"href\")\n",
    "        \n",
    "        #if link_tail is None, it's not a repo-- don't add it!\n",
    "        if link_tail is not None:\n",
    "            #add tail to list of repo names (get rid of '/' in front)\n",
    "            repo_names.append(link_tail[1:])\n",
    "            #add url of repo to list of repo urls\n",
    "            repo_url = url_head + link_tail\n",
    "            url_list.append(repo_url)\n",
    "\n",
    "    #get programming language\n",
    "    for repo in soup_thingy1.find_all(class_ = \"d-table-cell col-2 text-gray pt-2\"):\n",
    "        language = repo.get_text().strip() #strip white space\n",
    "        language_list.append(language)\n",
    "\n",
    "    #get number of stars\n",
    "    for repo in soup_thingy1.find_all(class_ = \"col-2 text-right pt-1 pr-3 pt-2\"):\n",
    "        #number of stars is embedded within <a> tag\n",
    "        stars = repo.find(\"a\").get_text().strip()\n",
    "        stars_list.append(stars)\n",
    "\n",
    "\n",
    "#printing/debugging\n",
    "print(\"url list\", len(url_list))\n",
    "print(\"names list\", len(repo_names))\n",
    "print(\"lang list\", len(language_list))\n",
    "print(\"stars list\", len(stars_list))\n",
    "#print(url_list)\n",
    "#print(repo_names)\n",
    "#print(language_list)\n",
    "#print(stars_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#convert repo urls to soup objs\n",
    "#we can only access 9 pages every minute before getting blocked\n",
    "\n",
    "repo_soup_list = []\n",
    "count = 0\n",
    "\n",
    "for i in range(len(url_list)):\n",
    "    repo_soup_list.append(url_to_soup(url_list[i]))\n",
    "    \n",
    "    # we increment the count until we reach 9\n",
    "    # then pause for 60 sec to prevent blocking\n",
    "    count += 1\n",
    "    if(count == 8):\n",
    "        sleep(60)\n",
    "        count = 0\n",
    "\n",
    "print(\"done\")\n",
    "#this takes about 16 minutes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(repo_soup_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## traverse ENTIRE list of repo soups objs and make lists of ALL 100 readme urls, issues, contributors, forks\n",
    "\n",
    "#lists to hold data\n",
    "issues_list = []\n",
    "contrib_list = []\n",
    "fork_list = []\n",
    "readme_list = []\n",
    "\n",
    "#loop through list of repo soup objs\n",
    "for i in range(len(repo_soup_list)):\n",
    "    repo_soup = repo_soup_list[i]\n",
    "    \n",
    "    ##get number of issues\n",
    "    for tag in repo_soup.find(class_=\"Counter\"):\n",
    "        issues_list.append(tag)\n",
    "\n",
    "    ##get number of contributers\n",
    "    contrib_numbers = []\n",
    "    #there are 4 data points in class \"num text-emphasized\"-- add them all to list\n",
    "    for tag in repo_soup.find_all(class_=\"num text-emphasized\"):\n",
    "        contrib_numbers.append(tag.get_text().strip())\n",
    "    #number of contribs is always 4th element-- let's grab it from our list\n",
    "    #if it's NoneType, it's not a contributor!\n",
    "    if contrib_numbers[3] is not None:\n",
    "        contrib_list.append(contrib_numbers[3])\n",
    "\n",
    "    ##get number of forks\n",
    "    fork_numbers = []\n",
    "    #there are 3 data points in class \"social-count\"-- add them all to list\n",
    "    for tag in repo_soup.find_all(class_=\"social-count\"):\n",
    "        fork_numbers.append(tag.get_text().strip())\n",
    "    #number of forks is always 3rd element-- let's grab it from our list\n",
    "    #if it's NoneType, it's not a fork!\n",
    "    if fork_numbers[2] is not None:\n",
    "        fork_list.append(fork_numbers[2])    \n",
    "      \n",
    "    ##get url for readme file\n",
    "    #find tag with href containing readme\n",
    "    if (repo_soup.find_all(\"a\", href=re.compile(\"README.md\")) == []):\n",
    "        if (repo_soup.find_all(\"a\", href=re.compile(\"blob/master/R\")) == []):\n",
    "            readme = repo_soup.find_all(\"a\", href=re.compile(\"blob/master/read\"))\n",
    "        else:\n",
    "            readme = repo_soup.find_all(\"a\", href=re.compile(\"blob/master/Read\"))\n",
    "    else:\n",
    "        readme = repo_soup.find_all(\"a\", href=re.compile(\"README\"))\n",
    "    #traverse the tag and grab the url \n",
    "    for tag in readme:\n",
    "        readme_tail = tag.get(\"href\")\n",
    "    #combine url with github url head for full link to readme file\n",
    "    #first check if url already contains github header\n",
    "    if \"https://github\" in readme_tail:\n",
    "        readme_url = readme_tail.split(\"#\")[0]\n",
    "    else:\n",
    "        readme_url = (url_head + readme_tail).split(\"#\")[0] #this gets rid of anything after and including hashtag\n",
    "    #print(readme_url)\n",
    "    #add to list of all readme urls\n",
    "    readme_list.append(readme_url)\n",
    "\n",
    "\n",
    "    \n",
    "##printing/debugging\n",
    "# print(readme_list)\n",
    "# print(issues_list)\n",
    "# print(contrib_list)\n",
    "# print(fork_list)\n",
    "print(len(issues_list))\n",
    "print(len(contrib_list))\n",
    "print(len(fork_list))\n",
    "print(len(readme_list))\n",
    "\n",
    "#reference for finding word in href\n",
    "#https://stackoverflow.com/questions/38252434/beautifulsoup-to-find-a-link-that-contains-a-specific-word\n",
    "\n",
    "#how to keep first item up to but not including \";\"\n",
    "#line.split(\";\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##change repeated urls manually-- it's just faster this way\n",
    "# 'https://github.com/mui-org/material-ui/blob/v1-beta/README.md' [53,54]\n",
    "readme_list[53] = 'https://github.com/pallets/flask/blob/master/README.rst'\n",
    "readme_list[54] = 'https://github.com/jekyll/jekyll/blob/master/README.markdown'\n",
    "\n",
    "# 'https://github.com/ripienaar/free-for-dev/blob/master/README.md' [63]\n",
    "readme_list[63] = 'https://github.com/django/django/blob/master/README.rst'\n",
    "\n",
    "# 'https://github.com/AFNetworking/AFNetworking/blob/master/README.md' [70]\n",
    "readme_list[70] = 'https://github.com/requests/requests/blob/master/README.rst'\n",
    "\n",
    "# 'https://github.com/ssshooter/MyDiary-Vue/blob/master/README.En.md'\n",
    "readme_list[79] = 'https://github.com/elastic/elasticsearch/blob/master/README.textile'\n",
    "\n",
    "##initiate list for readme soup objs\n",
    "readme_soup = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##traverse ENTIRE list of readme urls and make list of ALL 100 readme soup objs\n",
    "\n",
    "#run 9 every minute so we don't get blocked\n",
    "\n",
    "#make soup objects for each readme url\n",
    "for i in range(len(readme_list)):\n",
    "    #every 9th item, pause so we don't get blocked\n",
    "    if (i % 9 == 0):\n",
    "        sleep(60)\n",
    "    \n",
    "    readme_soup.append(url_to_soup(readme_list[i]))\n",
    "\n",
    "\n",
    "print(\"done\")\n",
    "print(len(readme_soup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# #debugging\n",
    "\n",
    "# #initiate variables to be used below\n",
    "# readme_soup_tester = []\n",
    "# count_tester = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# #debugging-- THIS GIVES CORRECT SOUP LIST! but manually ran for loop each time\n",
    "\n",
    "# # for i in range(9):\n",
    "# #     readme_soup_tester.append(url_to_soup(readme_list[i]))\n",
    "# #     count_tester += 1\n",
    "\n",
    "# # begin = count_tester\n",
    "# # end = count_tester + 9\n",
    "\n",
    "# #wait 60 sec\n",
    "\n",
    "# for i in range(begin,end):\n",
    "#     readme_soup_tester.append(url_to_soup(readme_list[i]))\n",
    "#     count_tester += 1\n",
    "    \n",
    "# begin = count_tester\n",
    "# end = count_tester + 9\n",
    "# if end > 100:\n",
    "#     end = 100\n",
    "\n",
    "\n",
    "# print(\"count\", count_tester)\n",
    "# print(\"list size\", len(readme_soup_tester))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## traverse ENTIRE list of readme soup objs and make list of ALL 100 readme file sizes\n",
    "\n",
    "readme_file_sizes = []\n",
    "\n",
    "#make list of readme file sizes\n",
    "for i in range(len(readme_soup)):\n",
    "    for tag in readme_soup[i].find_all(class_ = \"file-info\"):\n",
    "        info = tag.get_text().strip()\n",
    "        #print(info.splitlines())\n",
    "        #file size is the third element of file info\n",
    "        readme_size = info.splitlines()[2].strip()\n",
    "        #print(readme_size)\n",
    "        readme_file_sizes.append(readme_size)\n",
    "    \n",
    "\n",
    "#printing/debugging\n",
    "#print(readme_file_sizes)\n",
    "print(len(readme_file_sizes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-8cff483ecb6f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;31m# Convert lists into DataFrame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m repos_df = (pd.DataFrame({'Repo Name': repo_names, 'URL': url_list, 'Contributors': contrib_list, \n\u001b[0m\u001b[1;32m      4\u001b[0m                                    \u001b[1;34m'ProgrammingLanguage'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlanguage_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Stars'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstars_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Issues'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0missues_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                    'Forks': fork_list, 'Readme File Size': readme_file_sizes}))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# Convert lists into DataFrame\n",
    "\n",
    "repos_df = (pd.DataFrame({'Repo Name': repo_names, 'URL': url_list, 'Contributors': contrib_list, \n",
    "                                   'ProgrammingLanguage': language_list, 'Stars': stars_list, 'Issues': issues_list,\n",
    "                                   'Forks': fork_list, 'Readme File Size': readme_file_sizes}))\n",
    "\n",
    "#reorder columns\n",
    "repos_df = repos_df[['Repo Name', 'URL', 'Contributors', 'ProgrammingLanguage', 'Stars', 'Issues', 'Forks', 'Readme File Size']]\n",
    "\n",
    "#visually inspect df\n",
    "#repos_df.tail(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save dataframe to file project_info.csv\n",
    "\n",
    "repos_df.to_csv('project_info.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Use linear regression to analyze the Github repository data\n",
    "\n",
    "In this part, you will analyze the data collected in Part 1 using regression tools. The goal is to identify properties that make a repository popular. \n",
    "\n",
    "First, load the project_info.csv file in again. **We need you to do this so that we can run your code below withouth having to run your scraping code.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this loads the data from the project_info.csv file \n",
    "project_info = pd.read_csv('project_info.csv', index_col=0)\n",
    "\n",
    "#inspect data\n",
    "project_info.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2.1. Reformat the data into useful datatypes\n",
    "\n",
    "1. Print the data types for the columns of your dataframe\n",
    "+ Reformat the colums Issues, Forks, Stars, and Contributers to be integer data\n",
    "+ There is one repository flagged as having infinite contributers (the Linux kernel). We'll assume that it in fact has 15000 contributors (about twice as much as the next project). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##Task 2.1\n",
    "\n",
    "#reformat convert data type of stars column\n",
    "project_info['Stars'] = project_info['Stars'].str.replace('k','')\n",
    "project_info['Stars'] = project_info['Stars'].astype(float)\n",
    "project_info['Stars'] = project_info['Stars']*1000\n",
    "project_info['Stars'] = project_info['Stars'].astype(int)\n",
    "\n",
    "#reformat and convert data type of forks column\n",
    "project_info['Forks'] = project_info['Forks'].str.replace(',','')\n",
    "project_info['Forks'] = project_info['Forks'].astype(int)\n",
    "\n",
    "#reformat and convert data type of contributors column\n",
    "con_idx = project_info.index[project_info['Contributors'] == '?']\n",
    "project_info.loc[16, 'Contributors'] = 15000\n",
    "project_info['Contributors'] = project_info['Contributors'].str.replace(',','')\n",
    "project_info['Contributors'].fillna(0, inplace = True) #replace nan w/0\n",
    "project_info['Contributors'] = project_info['Contributors'].astype(int)\n",
    "\n",
    "#reformat and convert data type of issues column\n",
    "project_info['Issues'] = project_info['Issues'].str.replace(',','')\n",
    "project_info['Issues'] = project_info['Issues'].astype(int)\n",
    "\n",
    "#reformat and convert data type of readme file size\n",
    "#first split size (number) and size type (KB, Byte, etc) and make new columns for these\n",
    "project_info['ReadmeSize'], project_info['Readme Size (type)'] = project_info['Readme File Size'].str.split(' ', 1).str\n",
    "project_info['ReadmeSize'] = project_info['ReadmeSize'].astype(float)\n",
    "project_info['Readme Size (type)'] = project_info['Readme Size (type)'].astype(str)\n",
    "\n",
    "#omit the rows with readme sizes that are not KB (these are not typical)\n",
    "project_info = project_info[project_info['Readme Size (type)'] == \"KB\"] \n",
    "\n",
    "\n",
    "    \n",
    "#inspect data\n",
    "project_info.head(20)\n",
    "project_info.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2 Describe the data\n",
    "\n",
    "+ Get an overview of the data using the describe function.\n",
    "+ Compute the correlation matrix, visualize it with a heat map.\n",
    "+ Visualize the correlations by making a scatterplot matrix.\n",
    "+ Interprete what you see.\n",
    "\n",
    "You can re-use code from your previous homework here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Task 2.2a: describe data\n",
    "\n",
    "#descriptive stats of data\n",
    "project_info.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Task 2.2b: visualize correlations with matrix and heat map\n",
    "\n",
    "#correlation matrix (w/variables from Task 2.2a descriptive stats)\n",
    "project_info_sub1 = project_info[['Stars', 'Contributors', 'Issues', 'Forks', 'ReadmeSize']]\n",
    "project_info_sub1.corr()\n",
    "\n",
    "#plot the heatmap\n",
    "fig, ax = plt.subplots()\n",
    "heatmap_sub1 = ax.pcolor(project_info_sub1.corr(), cmap=plt.cm.Blues, vmin=-1, vmax=1)\n",
    "\n",
    "#invert the plot to have diagonal top left to bottom right\n",
    "ax.invert_yaxis()\n",
    "ax.xaxis.tick_top()\n",
    "\n",
    "#set tick marks\n",
    "ax.set_xticks(np.arange(len(list(project_info_sub1))) + .5, minor=False) # +.5 to center\n",
    "ax.set_yticks(np.arange(len(list(project_info_sub1))) + .5, minor=False)\n",
    "plt.xticks(rotation=45) #rotate x-labels\n",
    "\n",
    "#label the plot\n",
    "ax.set_xticklabels(project_info_sub1.columns, minor=False)\n",
    "ax.set_yticklabels(project_info_sub1.columns, minor=False)\n",
    "\n",
    "project_info_sub1.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Interpretation:** From our correlation matrix and heatmap of the quantitative variables (Stars, Contributors, Issues, Forks and Readme Size), we can identify which variables are most and least correlated with one another. Initially, we notice that, compared to a correlation of 1 (which we see along the diagonal), there are no other variables with particularly strong correlations to one another. Perhaps the strongest positive, non-diagonal correlations are between Forks and Stars, and between Contributors and all other variables (except for Readme Size). In fact, we see that the correlations between Readme Size and all other variables is practically non-existent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Linear regression\n",
    "\n",
    "1. First use linear regression to try to predict the number of Stars based on Forks, Contributors, Issues, and Readme Length. Explain why this is not a very good model by discussing the R-squared , F-statistic p-value, and coefficient  p-values. \n",
    "+ Develop another model which is better. Explain why it is better and interpret your results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Task 2.3: linear regression\n",
    "\n",
    "#Task 2.3.1: regress stars with Forks, Contributors, Issues, and Readme Size\n",
    "sm.ols(formula = \"Stars ~ Contributors + Issues + Forks + ReadmeSize\", data = project_info_sub1).fit().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your interpretation: Task 2.3.1**\n",
    "\n",
    "Our model regressing Stars with Forks, Contributors, Issues, and Readme Size is insufficient to say the least. The R-squared value, which measures the ratio of variability explained by our model, is a measly 0.221. That is, Forks, Contributors, Issues, and Readme Size only explain 22% of the variability of Stars. The F-statistic is very small, 2.07e4, indicating our model is in fact significant. When we consider the coefficient P-values of the variables, we notice only one variable, Forks, has a P-value less than any typical significance level .01 or .05. This indicates that only Forks has a relationship with Stars. The other variables, unfortunately, do not have any relationship with Stars. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Task 2.3.2a: regress stars with Forks\n",
    "\n",
    "sm.ols(formula = \"Stars ~ Forks\", data = project_info_sub1).fit().summary()\n",
    "\n",
    "#Task 2.3.2b: regress stars with Forks and Programming Language\n",
    "sm.ols(formula = \"Stars ~ Forks + ProgrammingLanguage\", data = project_info_sub1).fit().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Your interpretation:** Since Forks is the only quantitative variable with any relationship to Stars, we attempt to create a model regressing only Stars and Forks. This, however, yields an R-squared value less than that in our model from Task 2.3.1. Perhaps an alternative model that could yield more significant results would attempt to regress Stars with Forks and a categorical variable such as Programming Language. We might find that the type of Programming Language for the repository could be a major contributor to the number of stars the repository has. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
